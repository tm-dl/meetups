{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Colorisation_start.ipynb","version":"0.3.2","provenance":[{"file_id":"1uiaG3q-xJqKoXmjFtG3ZkYAz45zJaWq-","timestamp":1550860187919},{"file_id":"1OEwC8WxSPTK8jVME0KGhRvMv0OlrITRw","timestamp":1550838022720}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"nxwQz4FHwCc6","colab_type":"code","colab":{}},"cell_type":"code","source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import math\n","import time\n","\n","import tensorflow as tf\n","\n","# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","\n","# we will use Sonnet on top of TF \n","!pip install -q dm-sonnet\n","import sonnet as snt\n","\n","import numpy as np\n","import os\n","\n","# Plotting library.\n","from matplotlib import pyplot as plt\n","from skimage import color\n","import scipy\n","import sklearn.neighbors as nn\n","import pylab as pl\n","from IPython.display import clear_output, Image, display, HTML\n","\n","from google.colab import files"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TaQYiHNf6xRN","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OUTSTcN306YP","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Bin centres in ab space; 313 bins, with (a, b) coordinates ; shape (313, 2)\n","bin_centres = np.asarray([[ -90,   50],\n","       [ -90,   60],\n","       [ -90,   70],\n","       [ -90,   80],\n","       [ -90,   90],\n","       [ -80,   20],\n","       [ -80,   30],\n","       [ -80,   40],\n","       [ -80,   50],\n","       [ -80,   60],\n","       [ -80,   70],\n","       [ -80,   80],\n","       [ -80,   90],\n","       [ -70,    0],\n","       [ -70,   10],\n","       [ -70,   20],\n","       [ -70,   30],\n","       [ -70,   40],\n","       [ -70,   50],\n","       [ -70,   60],\n","       [ -70,   70],\n","       [ -70,   80],\n","       [ -70,   90],\n","       [ -60,  -20],\n","       [ -60,  -10],\n","       [ -60,    0],\n","       [ -60,   10],\n","       [ -60,   20],\n","       [ -60,   30],\n","       [ -60,   40],\n","       [ -60,   50],\n","       [ -60,   60],\n","       [ -60,   70],\n","       [ -60,   80],\n","       [ -60,   90],\n","       [ -50,  -30],\n","       [ -50,  -20],\n","       [ -50,  -10],\n","       [ -50,    0],\n","       [ -50,   10],\n","       [ -50,   20],\n","       [ -50,   30],\n","       [ -50,   40],\n","       [ -50,   50],\n","       [ -50,   60],\n","       [ -50,   70],\n","       [ -50,   80],\n","       [ -50,   90],\n","       [ -50,  100],\n","       [ -40,  -40],\n","       [ -40,  -30],\n","       [ -40,  -20],\n","       [ -40,  -10],\n","       [ -40,    0],\n","       [ -40,   10],\n","       [ -40,   20],\n","       [ -40,   30],\n","       [ -40,   40],\n","       [ -40,   50],\n","       [ -40,   60],\n","       [ -40,   70],\n","       [ -40,   80],\n","       [ -40,   90],\n","       [ -40,  100],\n","       [ -30,  -50],\n","       [ -30,  -40],\n","       [ -30,  -30],\n","       [ -30,  -20],\n","       [ -30,  -10],\n","       [ -30,    0],\n","       [ -30,   10],\n","       [ -30,   20],\n","       [ -30,   30],\n","       [ -30,   40],\n","       [ -30,   50],\n","       [ -30,   60],\n","       [ -30,   70],\n","       [ -30,   80],\n","       [ -30,   90],\n","       [ -30,  100],\n","       [ -20,  -50],\n","       [ -20,  -40],\n","       [ -20,  -30],\n","       [ -20,  -20],\n","       [ -20,  -10],\n","       [ -20,    0],\n","       [ -20,   10],\n","       [ -20,   20],\n","       [ -20,   30],\n","       [ -20,   40],\n","       [ -20,   50],\n","       [ -20,   60],\n","       [ -20,   70],\n","       [ -20,   80],\n","       [ -20,   90],\n","       [ -20,  100],\n","       [ -10,  -60],\n","       [ -10,  -50],\n","       [ -10,  -40],\n","       [ -10,  -30],\n","       [ -10,  -20],\n","       [ -10,  -10],\n","       [ -10,    0],\n","       [ -10,   10],\n","       [ -10,   20],\n","       [ -10,   30],\n","       [ -10,   40],\n","       [ -10,   50],\n","       [ -10,   60],\n","       [ -10,   70],\n","       [ -10,   80],\n","       [ -10,   90],\n","       [ -10,  100],\n","       [   0,  -70],\n","       [   0,  -60],\n","       [   0,  -50],\n","       [   0,  -40],\n","       [   0,  -30],\n","       [   0,  -20],\n","       [   0,  -10],\n","       [   0,    0],\n","       [   0,   10],\n","       [   0,   20],\n","       [   0,   30],\n","       [   0,   40],\n","       [   0,   50],\n","       [   0,   60],\n","       [   0,   70],\n","       [   0,   80],\n","       [   0,   90],\n","       [   0,  100],\n","       [  10,  -80],\n","       [  10,  -70],\n","       [  10,  -60],\n","       [  10,  -50],\n","       [  10,  -40],\n","       [  10,  -30],\n","       [  10,  -20],\n","       [  10,  -10],\n","       [  10,    0],\n","       [  10,   10],\n","       [  10,   20],\n","       [  10,   30],\n","       [  10,   40],\n","       [  10,   50],\n","       [  10,   60],\n","       [  10,   70],\n","       [  10,   80],\n","       [  10,   90],\n","       [  20,  -80],\n","       [  20,  -70],\n","       [  20,  -60],\n","       [  20,  -50],\n","       [  20,  -40],\n","       [  20,  -30],\n","       [  20,  -20],\n","       [  20,  -10],\n","       [  20,    0],\n","       [  20,   10],\n","       [  20,   20],\n","       [  20,   30],\n","       [  20,   40],\n","       [  20,   50],\n","       [  20,   60],\n","       [  20,   70],\n","       [  20,   80],\n","       [  20,   90],\n","       [  30,  -90],\n","       [  30,  -80],\n","       [  30,  -70],\n","       [  30,  -60],\n","       [  30,  -50],\n","       [  30,  -40],\n","       [  30,  -30],\n","       [  30,  -20],\n","       [  30,  -10],\n","       [  30,    0],\n","       [  30,   10],\n","       [  30,   20],\n","       [  30,   30],\n","       [  30,   40],\n","       [  30,   50],\n","       [  30,   60],\n","       [  30,   70],\n","       [  30,   80],\n","       [  30,   90],\n","       [  40, -100],\n","       [  40,  -90],\n","       [  40,  -80],\n","       [  40,  -70],\n","       [  40,  -60],\n","       [  40,  -50],\n","       [  40,  -40],\n","       [  40,  -30],\n","       [  40,  -20],\n","       [  40,  -10],\n","       [  40,    0],\n","       [  40,   10],\n","       [  40,   20],\n","       [  40,   30],\n","       [  40,   40],\n","       [  40,   50],\n","       [  40,   60],\n","       [  40,   70],\n","       [  40,   80],\n","       [  40,   90],\n","       [  50, -100],\n","       [  50,  -90],\n","       [  50,  -80],\n","       [  50,  -70],\n","       [  50,  -60],\n","       [  50,  -50],\n","       [  50,  -40],\n","       [  50,  -30],\n","       [  50,  -20],\n","       [  50,  -10],\n","       [  50,    0],\n","       [  50,   10],\n","       [  50,   20],\n","       [  50,   30],\n","       [  50,   40],\n","       [  50,   50],\n","       [  50,   60],\n","       [  50,   70],\n","       [  50,   80],\n","       [  60, -110],\n","       [  60, -100],\n","       [  60,  -90],\n","       [  60,  -80],\n","       [  60,  -70],\n","       [  60,  -60],\n","       [  60,  -50],\n","       [  60,  -40],\n","       [  60,  -30],\n","       [  60,  -20],\n","       [  60,  -10],\n","       [  60,    0],\n","       [  60,   10],\n","       [  60,   20],\n","       [  60,   30],\n","       [  60,   40],\n","       [  60,   50],\n","       [  60,   60],\n","       [  60,   70],\n","       [  60,   80],\n","       [  70, -110],\n","       [  70, -100],\n","       [  70,  -90],\n","       [  70,  -80],\n","       [  70,  -70],\n","       [  70,  -60],\n","       [  70,  -50],\n","       [  70,  -40],\n","       [  70,  -30],\n","       [  70,  -20],\n","       [  70,  -10],\n","       [  70,    0],\n","       [  70,   10],\n","       [  70,   20],\n","       [  70,   30],\n","       [  70,   40],\n","       [  70,   50],\n","       [  70,   60],\n","       [  70,   70],\n","       [  70,   80],\n","       [  80, -110],\n","       [  80, -100],\n","       [  80,  -90],\n","       [  80,  -80],\n","       [  80,  -70],\n","       [  80,  -60],\n","       [  80,  -50],\n","       [  80,  -40],\n","       [  80,  -30],\n","       [  80,  -20],\n","       [  80,  -10],\n","       [  80,    0],\n","       [  80,   10],\n","       [  80,   20],\n","       [  80,   30],\n","       [  80,   40],\n","       [  80,   50],\n","       [  80,   60],\n","       [  80,   70],\n","       [  90, -110],\n","       [  90, -100],\n","       [  90,  -90],\n","       [  90,  -80],\n","       [  90,  -70],\n","       [  90,  -60],\n","       [  90,  -50],\n","       [  90,  -40],\n","       [  90,  -30],\n","       [  90,  -20],\n","       [  90,  -10],\n","       [  90,    0],\n","       [  90,   10],\n","       [  90,   20],\n","       [  90,   30],\n","       [  90,   40],\n","       [  90,   50],\n","       [  90,   60],\n","       [  90,   70],\n","       [ 100,  -90],\n","       [ 100,  -80],\n","       [ 100,  -70],\n","       [ 100,  -60],\n","       [ 100,  -50],\n","       [ 100,  -40],\n","       [ 100,  -30],\n","       [ 100,  -20],\n","       [ 100,  -10],\n","       [ 100,    0]])\n","print (bin_centres.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dt9zhilAvU4b","colab_type":"code","colab":{}},"cell_type":"code","source":["K = bin_centres.shape[0]\n","num_outputs = K"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qWdxfxk21EH2","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Weights to boost rare colours; shape (313,)\n","prior_weights = np.asarray([  4.88637077e-08,   1.38517899e-07,   5.76575330e-07,\n","         2.63933821e-06,   9.50312428e-07,   3.56691466e-08,\n","         2.53701218e-07,   7.52531286e-07,   1.96176670e-06,\n","         3.01480592e-06,   4.88644584e-06,   5.64053403e-06,\n","         1.29429273e-06,   9.36679821e-08,   6.35391447e-07,\n","         2.24641415e-06,   5.25553580e-06,   8.76607414e-06,\n","         1.34091655e-05,   1.82779229e-05,   1.60724964e-05,\n","         7.22202335e-06,   1.19142890e-06,   4.18887782e-07,\n","         1.88786671e-06,   5.56257122e-06,   1.39430890e-05,\n","         2.40792026e-05,   3.47403683e-05,   4.82280696e-05,\n","         6.84466539e-05,   7.22763768e-05,   4.29625157e-05,\n","         1.64851003e-05,   3.02568434e-06,   7.73805166e-07,\n","         1.64461151e-05,   4.38741086e-05,   5.87340628e-05,\n","         8.72316315e-05,   1.18770632e-04,   1.67088961e-04,\n","         2.63252494e-04,   3.38518453e-04,   2.60347030e-04,\n","         1.26975045e-04,   4.08961754e-05,   7.22817564e-06,\n","         1.99693873e-07,   8.92095222e-07,   2.17681063e-05,\n","         1.12513049e-04,   2.16379538e-04,   2.66030577e-04,\n","         3.32276125e-04,   5.05886131e-04,   9.30736644e-04,\n","         1.37460574e-03,   1.22399826e-03,   6.84941967e-04,\n","         2.66331750e-04,   7.81269159e-05,   1.51248334e-05,\n","         6.91909015e-07,   7.84442407e-07,   2.60442924e-05,\n","         1.73225904e-04,   4.26183231e-04,   6.94351438e-04,\n","         9.10106607e-04,   1.36396435e-03,   2.65613957e-03,\n","         4.28549468e-03,   4.12207952e-03,   2.46151934e-03,\n","         1.05696835e-03,   3.65279981e-04,   1.16697960e-04,\n","         3.93379465e-05,   5.51556883e-06,   2.31736474e-05,\n","         2.18864360e-04,   7.00249522e-04,   1.39982571e-03,\n","         2.46769627e-03,   4.12544004e-03,   6.65952684e-03,\n","         9.48145351e-03,   9.47151961e-03,   6.01423441e-03,\n","         2.73632660e-03,   1.05931639e-03,   4.24813148e-04,\n","         2.06608970e-04,   1.12516660e-04,   1.82343798e-05,\n","         1.29806207e-05,   1.98459648e-04,   1.02348520e-03,\n","         3.02121005e-03,   6.83473237e-03,   1.91548207e-02,\n","         4.91089070e-02,   3.73436164e-02,   2.15410263e-02,\n","         1.19576249e-02,   5.37496020e-03,   2.22045083e-03,\n","         1.00228369e-03,   5.34522164e-04,   2.98875772e-04,\n","         1.13312553e-04,   7.14431138e-06,   5.09891097e-06,\n","         1.17016968e-04,   7.91608776e-04,   2.85160331e-03,\n","         6.63106003e-03,   1.52823814e-02,   6.42029222e-02,\n","         2.01234011e-01,   1.08298509e-01,   4.15076383e-02,\n","         1.65514759e-02,   6.17025338e-03,   2.49037787e-03,\n","         1.17210899e-03,   6.26213708e-04,   3.19615929e-04,\n","         7.25174444e-05,   1.56894049e-06,   1.36853721e-06,\n","         4.80932224e-05,   3.54808240e-04,   1.19859673e-03,\n","         2.42655886e-03,   3.77852526e-03,   6.61168484e-03,\n","         1.95601464e-02,   5.31084300e-02,   4.83594842e-02,\n","         3.27312450e-02,   1.76505654e-02,   7.57483162e-03,\n","         3.07662489e-03,   1.31642943e-03,   6.08919745e-04,\n","         2.51085095e-04,   2.98835289e-05,   1.62990511e-05,\n","         1.50673550e-04,   4.48743165e-04,   8.11259090e-04,\n","         1.02889008e-03,   1.07089813e-03,   1.19569058e-03,\n","         2.19157211e-03,   5.41470406e-03,   1.05136718e-02,\n","         1.22407698e-02,   9.83720010e-03,   5.75596418e-03,\n","         2.78841071e-03,   1.22907076e-03,   5.18767129e-04,\n","         1.60173838e-04,   8.73308520e-06,   3.67741205e-06,\n","         6.01862290e-05,   1.95014172e-04,   3.29027511e-04,\n","         3.92108962e-04,   3.36467202e-04,   2.87580737e-04,\n","         3.55864982e-04,   6.00025038e-04,   1.30302957e-03,\n","         2.94264848e-03,   4.35510075e-03,   4.23253106e-03,\n","         3.10584698e-03,   1.85082460e-03,   9.27590744e-04,\n","         3.96521780e-04,   8.90297299e-05,   2.16968519e-06,\n","         5.71286175e-07,   2.03625835e-05,   9.38233966e-05,\n","         1.53400572e-04,   1.73289582e-04,   1.39873962e-04,\n","         1.05413549e-04,   1.24053600e-04,   1.68733854e-04,\n","         2.72700806e-04,   4.94871924e-04,   9.93266911e-04,\n","         1.84474766e-03,   2.31814111e-03,   1.80826094e-03,\n","         1.15884412e-03,   6.29566586e-04,   2.67367592e-04,\n","         3.98110075e-05,   4.21985639e-07,   4.98443370e-06,\n","         4.41126865e-05,   8.27534031e-05,   7.93993301e-05,\n","         5.43451073e-05,   4.12448289e-05,   4.86518392e-05,\n","         6.68120367e-05,   9.23086078e-05,   1.48979560e-04,\n","         2.44392940e-04,   4.32722144e-04,   8.18053937e-04,\n","         1.39190683e-03,   1.54712315e-03,   9.45958739e-04,\n","         4.47192968e-04,   1.68976619e-04,   1.54429623e-05,\n","         8.54773467e-07,   1.67927601e-05,   4.30846659e-05,\n","         3.90191681e-05,   2.08583312e-05,   1.52734877e-05,\n","         1.76861134e-05,   2.59901753e-05,   3.63725347e-05,\n","         5.40605908e-05,   8.49764276e-05,   1.25964535e-04,\n","         1.94377266e-04,   3.54922552e-04,   6.60036129e-04,\n","         1.02443401e-03,   9.73277882e-04,   4.20818025e-04,\n","         1.09543742e-04,   5.62733417e-06,   5.84478616e-06,\n","         2.46368486e-05,   1.78842817e-05,   7.63802784e-06,\n","         5.27525552e-06,   5.87004194e-06,   9.44847952e-06,\n","         1.41464633e-05,   1.89742307e-05,   2.83317550e-05,\n","         4.17858199e-05,   5.53431612e-05,   7.32897293e-05,\n","         1.17445560e-04,   2.17510161e-04,   3.73459793e-04,\n","         5.23290004e-04,   3.94743063e-04,   9.10961481e-05,\n","         2.36171766e-06,   7.43685027e-06,   9.79349898e-06,\n","         3.56092646e-06,   2.23531626e-06,   2.22702456e-06,\n","         2.89101214e-06,   5.42782588e-06,   6.57866549e-06,\n","         8.08947713e-06,   1.07832731e-05,   1.43150989e-05,\n","         1.61773707e-05,   1.76172387e-05,   2.22420318e-05,\n","         3.43043821e-05,   5.67338499e-05,   9.24106902e-05,\n","         1.34054343e-04,   5.91858296e-05,   3.78755066e-07,\n","         4.58242862e-07,   4.81465701e-07,   6.92722895e-07,\n","         1.01984763e-06,   2.34830496e-06,   2.92547680e-06,\n","         2.83353631e-06,   2.48962571e-06,   2.08168930e-06,\n","         2.12280955e-06,   1.66435596e-06,   1.28913095e-06,\n","         1.11413320e-06,   1.22927291e-06,   1.59697061e-06,\n","         2.46489627e-06,   5.07245642e-06,   3.65973502e-06,\n","         6.07144475e-09,   3.65559565e-08,   2.90157510e-07,\n","         1.40430462e-06,   6.00511564e-07,   2.64900940e-07,\n","         1.20902155e-07,   4.78526910e-08,   2.88152932e-08,\n","         1.45039504e-08])\n","print (prior_weights.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AAHoVUxV3_wP","colab_type":"code","colab":{}},"cell_type":"code","source":["dataset_name = 'tiny_imagenet'  #@param\n","image_size = 64  #@param\n","batch_size = 12  #@param\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lTvzohVwy17r","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Preprocess data (RGB -> Lab -> classification embedding and back)\n","def flatten_nd_array(pts_nd, axis=1):\n","  ''' Flatten an nd array into a 2d array with a certain axis\n","  INPUTS\n","      pts_nd       N0xN1x...xNd array\n","      axis         integer\n","  OUTPUTS\n","      pts_flt     prod(N \\ N_axis) x N_axis array     '''\n","  NDIM = pts_nd.ndim\n","  SHP = np.array(pts_nd.shape)\n","  nax = np.setdiff1d(np.arange(0, NDIM), np.array((axis))) # non axis indices\n","  NPTS = np.prod(SHP[nax])\n","  axorder = np.concatenate((nax, np.array(axis).flatten()), axis=0)\n","  pts_flt = pts_nd.transpose((axorder))\n","  pts_flt = pts_flt.reshape(NPTS, SHP[axis])\n","  return pts_flt\n","\n","\n","def unflatten_2d_array(pts_flt, pts_nd, axis=1, squeeze=False):\n","  ''' Unflatten a 2d array with a certain axis\n","  INPUTS\n","      pts_flt     prod(N \\ N_axis) x M array\n","      pts_nd      N0xN1x...xNd array\n","      axis        integer\n","      squeeze     bool     if true, M=1, squeeze it out\n","  OUTPUTS\n","      pts_out     N0xN1x...xNd array        '''\n","  NDIM = pts_nd.ndim\n","  SHP = np.array(pts_nd.shape)\n","  nax = np.setdiff1d(np.arange(0, NDIM), np.array((axis))) # non axis indices\n","  NPTS = np.prod(SHP[nax])\n","\n","  if(squeeze):\n","    axorder = nax\n","    axorder_rev = np.argsort(axorder)\n","    M = pts_flt.shape[1]\n","    NEW_SHP = SHP[nax].tolist()\n","    pts_out = pts_flt.reshape(NEW_SHP)\n","    pts_out = pts_out.transpose(axorder_rev)\n","  else:\n","    axorder = np.concatenate((nax, np.array(axis).flatten()),axis=0)\n","    axorder_rev = np.argsort(axorder)\n","    M = pts_flt.shape[1]\n","    NEW_SHP = SHP[nax].tolist()\n","    NEW_SHP.append(M)\n","    pts_out = pts_flt.reshape(NEW_SHP)\n","    pts_out = pts_out.transpose(axorder_rev)\n","\n","  return pts_out\n","\n","\n","def na(): # shorthand for new axis\n","  return np.newaxis\n","\n","\n","def prior_boost(data_ab_quant, axis=1):\n","  alpha = 1.\n","  gamma = .5\n","\n","  # empirical prior probability\n","  prior_probs = prior_weights\n","\n","  # define uniform probability\n","  uni_probs = np.zeros_like(prior_probs)\n","  uni_probs[prior_probs != 0] = 1.\n","  uni_probs = uni_probs / np.sum(uni_probs)\n","\n","  # convex combination of empirical prior and uniform distribution       \n","  prior_mix = (1 - gamma) * prior_probs + gamma * uni_probs\n","\n","  # set prior factor\n","  prior_factor = prior_mix**-alpha\n","  prior_factor = prior_factor / np.sum(prior_probs * prior_factor) # re-normalize\n","\n","  # implied empirical prior\n","  implied_prior = prior_probs * prior_factor\n","  implied_prior = implied_prior / np.sum(implied_prior) # re-normalize\n","  \n","  data_ab_maxind = np.argmax(data_ab_quant, axis=axis)\n","  corr_factor = prior_factor[data_ab_maxind]\n","  if(axis==0):\n","    return corr_factor[na(),:]\n","  elif(axis==1):\n","    return corr_factor[:,na(),:]\n","  elif(axis==2):\n","    return corr_factor[:,:,na(),:]\n","  elif(axis==3):\n","    return corr_factor[:,:,:,na()]\n","\n","\n","def py_preprocess_images_labels(rgb_images, axis=1):\n","  \"\"\"Convert rgb images to Lab and encode ab in bin-space with boosting.\"\"\"\n","  num_nearest_neighbours = int(10)\n","  sigma = 5.\n","  nbrs = nn.NearestNeighbors(n_neighbors=num_nearest_neighbours, algorithm='ball_tree').fit(bin_centres)\n","  lab_images = color.rgb2lab(rgb_images)\n","  L = lab_images[:, :, :, 0]\n","  L -= 50. # centre the data\n","  ab_pts = lab_images[:, :, :, 1:]\n","  thresh = 5.\n","  nongray_mask = (np.sum(np.sum(np.sum(np.abs(ab_pts) > thresh, axis=1), axis=1), axis=1) > 0)[:, na(), na()]\n","  ab_pts = np.transpose(ab_pts, [0, 3, 1, 2])\n","  pts_flt = flatten_nd_array(ab_pts, axis=axis)\n","  P = pts_flt.shape[0]\n","  pts_enc_flt = np.zeros((P, K))\n","  p_inds = np.arange(0, P, dtype='int')[:,na()]\n","\n","  P = pts_flt.shape[0]\n","  (dists, inds) = nbrs.kneighbors(pts_flt)\n","\n","  wts = np.exp(-dists**2 / (2 * sigma**2))\n","  wts = wts / np.sum(wts, axis=1)[:, na()]\n","\n","  pts_enc_flt[p_inds, inds] = wts\n","  pts_enc = unflatten_2d_array(pts_enc_flt, ab_pts, axis=axis)\n","  del pts_enc_flt\n","  boost_pts_enc = prior_boost(pts_enc)\n","  pts_enc = np.transpose(pts_enc, [0, 2, 3, 1])\n","  boost_pts_enc = np.transpose(boost_pts_enc, [0, 2, 3, 1])\n","  boost_pts_enc = np.squeeze(boost_pts_enc, axis=-1)\n","  prior_boost_nongray = boost_pts_enc * nongray_mask\n","  L = np.expand_dims(L, axis=-1)\n","  return L, pts_enc, prior_boost_nongray \n","\n","\n","def embedding2RGB(L, pts_enc_nd, rebalance=1., axis=1):\n","  L += 50.\n","  def softmax(x):\n","    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n","    e_x = np.exp(x - np.expand_dims(np.max(x, axis=-1), axis=-1))\n","    return e_x / np.expand_dims(e_x.sum(axis=-1), axis=-1)\n","  \n","  pts_enc_nd *= rebalance\n","  pts_enc_nd = softmax(pts_enc_nd)\n","  pts_enc_nd = np.transpose(pts_enc_nd, [0, 3, 1, 2])\n","  \n","  pts_enc_flt = flatten_nd_array(pts_enc_nd, axis=axis)\n","  pts_dec_flt = np.dot(pts_enc_flt, bin_centres)\n","  pts_dec_nd = unflatten_2d_array(pts_dec_flt, pts_enc_nd, axis=axis)\n","  pts_dec_nd = np.transpose(pts_dec_nd, [0, 2, 3, 1])\n","    \n","  batch_size = L.shape[0]\n","  im_RGB = []\n","  for i in range(batch_size):\n","    pts_dec_nd1 = pts_dec_nd[i, :, :, 0]\n","    pts_dec_nd1 = pts_dec_nd1.repeat(2, axis=0).repeat(2, axis=1)  \n","    pts_dec_nd1 = np.expand_dims(pts_dec_nd1, axis=-1)\n","    pts_dec_nd2 = pts_dec_nd[i, :, :, 1]\n","    pts_dec_nd2 = pts_dec_nd2.repeat(2, axis=0).repeat(2, axis=1) \n","    pts_dec_nd2 = np.expand_dims(pts_dec_nd2, axis=-1)\n","    pts_Lab = np.concatenate([L[i], pts_dec_nd1, pts_dec_nd2], axis=-1) \n","    im_RGB.append((255*np.clip(color.lab2rgb(pts_Lab), 0, 1)).astype('uint8'))\n","  im_RGB = np.stack(im_RGB, axis=0)\n","  return im_RGB"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Zre38nTnwUDK","colab_type":"code","colab":{}},"cell_type":"code","source":["# Reset graph\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bsWdzhnW1ujj","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Prepare the data loaders for training and validation\n","def tiny_imagenet_parser(value, image_size, is_training):\n","  \"\"\"Parses tiny imagenet example.\n","\n","  Args:\n","    value: encoded example.\n","    image_size: size of the image.\n","    is_training: if True then do training preprocessing (which includes\n","      random cropping), otherwise do eval preprocessing.\n","\n","  Returns:\n","    image: tensor with the image.\n","    label: true label of the image.\n","  \"\"\"\n","  keys_to_features = {\n","      'image/encoded': tf.FixedLenFeature((), tf.string, ''),\n","      'label/tiny_imagenet': tf.FixedLenFeature([], tf.int64, -1),\n","  }\n","\n","  parsed = tf.parse_single_example(value, keys_to_features)\n","\n","  image_buffer = tf.reshape(parsed['image/encoded'], shape=[])\n","  image = tf.image.decode_image(image_buffer, channels=3)\n","  image = tf.image.convert_image_dtype(\n","      image, dtype=tf.float32)\n","\n","  # Crop image\n","  if is_training:\n","    bbox_begin, bbox_size, _ = tf.image.sample_distorted_bounding_box(\n","        tf.shape(image),\n","        bounding_boxes=tf.constant([0.0, 0.0, 1.0, 1.0],\n","                                   dtype=tf.float32,\n","                                   shape=[1, 1, 4]),\n","        min_object_covered=0.5,\n","        aspect_ratio_range=[0.75, 1.33],\n","        area_range=[0.5, 1.0],\n","        max_attempts=20,\n","        use_image_if_no_bounding_boxes=True)\n","    image = tf.slice(image, bbox_begin, bbox_size)\n","\n","  # resize image\n","  image = tf.image.resize_bicubic([image], [image_size, image_size])[0]\n","\n","  # Rescale image to [-1, 1] range.\n","  image = tf.multiply(tf.subtract(image, 0.5), 2.0)\n","\n","  image = tf.reshape(image, [image_size, image_size, 3])\n","\n","  # Labels are in [0, 199] range\n","  label = tf.cast(\n","      tf.reshape(parsed['label/tiny_imagenet'], shape=[]), dtype=tf.int32)\n","\n","  return image, label\n","\n","\n","def tiny_imagenet_input(split, batch_size, image_size, is_training, tiny_imagenet_data_dir=''):\n","  \"\"\"Returns Tiny Imagenet Dataset.\n","\n","  Args:\n","    split: name of the split, \"train\" or \"validation\".\n","    batch_size: size of the minibatch.\n","    image_size: size of the one side of the image. Output images will be\n","      resized to square shape image_size*image_size.\n","    is_training: if True then training preprocessing is done, otherwise eval\n","      preprocessing is done.instance of tf.data.Dataset with the dataset.\n","\n","  Raises:\n","    ValueError: if name of the split is incorrect.\n","\n","  Returns:\n","    Instance of tf.data.Dataset with the dataset.\n","  \"\"\"\n","  if split.lower().startswith('train'):\n","    filepath = os.path.join(tiny_imagenet_data_dir, 'train.tfrecord')\n","  elif split.lower().startswith('validation'):\n","    filepath = os.path.join(tiny_imagenet_data_dir, 'validation.tfrecord')\n","  else:\n","    raise ValueError('Invalid split: %s' % split)\n","\n","  dataset = tf.data.TFRecordDataset(filepath, buffer_size=8*1024*1024)\n","\n","  if is_training:\n","    dataset = dataset.shuffle(10000)\n","    dataset = dataset.repeat()\n","\n","  dataset = dataset.apply(\n","      tf.contrib.data.map_and_batch(\n","          lambda value: tiny_imagenet_parser(value, image_size, is_training),\n","          batch_size=batch_size,\n","          num_parallel_batches=4,\n","          drop_remainder=True))\n","\n","  def set_shapes(images, labels):\n","    \"\"\"Statically set the batch_size dimension.\"\"\"\n","    images.set_shape(images.get_shape().merge_with(\n","        tf.TensorShape([batch_size, None, None, None])))\n","    labels.set_shape(labels.get_shape().merge_with(\n","        tf.TensorShape([batch_size])))\n","    return images, labels\n","\n","  # Assign static batch size dimension\n","  dataset = dataset.map(set_shapes)\n","\n","  dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n","\n","  return dataset\n","\n","\n","def num_examples_per_epoch(split):\n","  \"\"\"Returns the number of examples in the data set.\n","\n","  Args:\n","    split: name of the split, \"train\" or \"validation\".\n","\n","  Raises:\n","    ValueError: if split name is incorrect.\n","\n","  Returns:\n","    Number of example in the split.\n","  \"\"\"\n","  if split.lower().startswith('train'):\n","    return 100000\n","  elif split.lower().startswith('validation'):\n","    return 10000\n","  else:\n","    raise ValueError('Invalid split: %s' % split)\n","\n","\n","def get_dataset(dataset_name, split, batch_size, image_size, is_training, tiny_imagenet_data_dir):\n","  \"\"\"Returns dataset.\n","\n","  Args:\n","    dataset_name: name of the dataset, only \"tiny_imagenet\" available here.\n","    split: name of the split, \"train\" or \"validation\".\n","    batch_size: size of the minibatch.\n","    image_size: size of the one side of the image. Output images will be\n","      resized to square shape image_size*image_size.\n","    is_training: if True then training preprocessing is done, otherwise eval\n","      preprocessing is done.\n","\n","  Raises:\n","    ValueError: if dataset_name is invalid.\n","\n","  Returns:\n","    dataset: instance of tf.data.Dataset with the dataset.\n","    num_examples: number of examples in given split of the dataset.\n","    num_classes: number of classes in the dataset.\n","    bounds: tuple with bounds of image values. All returned image pixels\n","      are between bounds[0] and bounds[1].\n","  \"\"\"\n","  if dataset_name == 'tiny_imagenet':\n","    dataset = tiny_imagenet_input(split, batch_size, image_size, is_training, tiny_imagenet_data_dir)\n","    num_examples = num_examples_per_epoch(split)\n","    num_classes = 200\n","    bounds = (-1, 1)\n","  else:\n","    raise ValueError('Invalid dataset %s' % dataset_name)\n","  return dataset, num_examples, num_classes, bounds"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Gi9zB3cASS7-","colab_type":"code","colab":{}},"cell_type":"code","source":["tiny_imagenet_data_dir = '/content/drive/My Drive/TMLW2019/TinyImagenet200/' #@param\n","# content/drive/My Drive/TMLW2019/TinyImagenet200/train.tfrecord"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1y4Zo34s2jjD","colab_type":"code","colab":{}},"cell_type":"code","source":["dataset_train, examples_per_epoch_train, num_classes, bounds = get_dataset(\n","  dataset_name, 'train', batch_size, image_size, True, tiny_imagenet_data_dir)\n","dataset_train_iterator = dataset_train.make_one_shot_iterator()\n","images_train, labels_train = dataset_train_iterator.get_next()\n","# make sure images are in [-1, 1]\n","images_train = tf.clip_by_value(images_train, -1., 1.)\n","# normalise to [0, 1]\n","images_train = (images_train + 1.0)/2.0\n","\n","dataset_valid, examples_per_epoch_valid, _, _ = get_dataset(dataset_name,\n","  'validation', batch_size, image_size, False, tiny_imagenet_data_dir)\n","dataset_valid_iterator = dataset_valid.make_one_shot_iterator()\n","images_valid, labels_valid = dataset_valid_iterator.get_next()\n","images_valid = tf.clip_by_value(images_valid, -1., 1.)\n","images_valid = (images_valid + 1.0)/2.0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E5qiOQpz5BZ_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Check sizes of tensors\n","print ('Tensor training images')\n","print (images_train)\n","print ('Tensor training labels')\n","print (labels_train)\n","print ('Tensor validation images')\n","print (images_valid)\n","print ('Tensor validation labels')\n","print (labels_valid)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Aykg1Xxf5hou","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Display images\n","MAX_IMAGES = 10\n","def gallery(images, label, title='Input images'):  \n","  class_dict = [\n","    u'Egyptian cat',\n","    u'reel',\n","    u'volleyball',\n","    u'rocking chair',\n","    u'lemon',\n","    u'bullfrog',\n","    u'basketball',\n","    u'cliff',\n","    u'espresso',\n","    u'plunger',\n","    u'parking meter',\n","    u'German shepherd',\n","    u'dining table',\n","    u'monarch',\n","    u'brown bear',\n","    u'school bus',\n","    u'pizza',\n","    u'guinea pig',\n","    u'umbrella',\n","    u'organ',\n","    u'oboe',\n","    u'maypole',\n","    u'goldfish',\n","    u'potpie',\n","    u'hourglass',\n","    u'seashore',\n","    u'computer keyboard',\n","    u'Arabian camel',\n","    u'ice cream',\n","    u'nail',\n","    u'space heater',\n","    u'cardigan',\n","    u'baboon',\n","    u'snail',\n","    u'coral reef',\n","    u'albatross',\n","    u'spider web',\n","    u'sea cucumber',\n","    u'backpack',\n","    u'Labrador',\n","    u'pretzel',\n","    u'king penguin',\n","    u'sulphur butterfly',\n","    u'tarantula',\n","    u'lesser panda',\n","    u'pop bottle',\n","    u'banana',\n","    u'sock',\n","    u'cockroach',\n","    u'projectile',\n","    u'beer',\n","    u'mantis',\n","    u'freight car',\n","    u'guacamole',\n","    u'remote control',\n","    u'European fire salamander',\n","    u'lakeside',\n","    u'chimpanzee',\n","    u'pay-phone',\n","    u'fur coat',\n","    u'alp',\n","    u'lampshade',\n","    u'torch',\n","    u'abacus',\n","    u'moving van',\n","    u'barrel',\n","    u'tabby cat',\n","    u'goose',\n","    u'koala',\n","    u'bullet train',\n","    u'CD player',\n","    u'teapot',\n","    u'birdhouse',\n","    u'gazelle',\n","    u'academic gown',\n","    u'tractor',\n","    u'ladybug',\n","    u'miniskirt',\n","    u'golden retriever',\n","    u'triumphal arch',\n","    u'cannon',\n","    u'neck brace',\n","    u'sombrero',\n","    u'gasmask',\n","    u'candle',\n","    u'desk',\n","    u'frying pan',\n","    u'bee',\n","    u'dam',\n","    u'spiny lobster',\n","    u'police van',\n","    u'iPod',\n","    u'punching bag',\n","    u'beacon',\n","    u'jellyfish',\n","    u'wok',\n","    u'potter wheel',\n","    u'sandal',\n","    u'pill bottle',\n","    u'butcher shop',\n","    u'slug',\n","    u'hog',\n","    u'cougar',\n","    u'crane',\n","    u'vestment',\n","    u'dragonfly',\n","    u'cash machine',\n","    u'mushroom',\n","    u'jinrikisha',\n","    u'water tower',\n","    u'chest',\n","    u'snorkel',\n","    u'sunglasses',\n","    u'fly',\n","    u'limousine',\n","    u'black stork',\n","    u'dugong',\n","    u'sports car',\n","    u'water jug',\n","    u'suspension bridge',\n","    u'ox',\n","    u'ice lolly',\n","    u'turnstile',\n","    u'Christmas stocking',\n","    u'broom',\n","    u'scorpion',\n","    u'wooden spoon',\n","    u'picket fence',\n","    u'rugby ball',\n","    u'sewing machine',\n","    u'steel arch bridge',\n","    u'Persian cat',\n","    u'refrigerator',\n","    u'barn',\n","    u'apron',\n","    u'Yorkshire terrier',\n","    u'swimming trunks',\n","    u'stopwatch',\n","    u'lawn mower',\n","    u'thatch',\n","    u'fountain',\n","    u'black widow',\n","    u'bikini',\n","    u'plate',\n","    u'teddy bear',\n","    u'barbershop',\n","    u'confectionery',\n","    u'beach wagon',\n","    u'scoreboard',\n","    u'orange',\n","    u'flagpole',\n","    u'American lobster',\n","    u'trolleybus',\n","    u'drumstick',\n","    u'dumbbell',\n","    u'brass',\n","    u'bow tie',\n","    u'convertible',\n","    u'bighorn',\n","    u'orangutan',\n","    u'American alligator',\n","    u'centipede',\n","    u'syringe',\n","    u'go-kart',\n","    u'brain coral',\n","    u'sea slug',\n","    u'cliff dwelling',\n","    u'mashed potato',\n","    u'viaduct',\n","    u'military uniform',\n","    u'pomegranate',\n","    u'chain',\n","    u'kimono',\n","    u'comic book',\n","    u'trilobite',\n","    u'bison',\n","    u'pole',\n","    u'boa constrictor',\n","    u'poncho',\n","    u'bathtub',\n","    u'grasshopper',\n","    u'walking stick',\n","    u'Chihuahua',\n","    u'tailed frog',\n","    u'lion',\n","    u'altar',\n","    u'obelisk',\n","    u'beaker',\n","    u'bell pepper',\n","    u'bannister',\n","    u'bucket',\n","    u'magnetic compass',\n","    u'meat loaf',\n","    u'gondola',\n","    u'standard poodle',\n","    u'acorn',\n","    u'lifeboat',\n","    u'binoculars',\n","    u'cauliflower',\n","    u'African elephant']\n","  batch_size, h, w, num_channels = images.shape\n","  batch_size = min(batch_size, MAX_IMAGES)\n","  ff, axes = plt.subplots(1, batch_size,\n","                          figsize=(2*batch_size, 1),\n","                          subplot_kw={'xticks': [], 'yticks': []})\n","  for i in range(0, batch_size):\n","    if num_channels == 3:\n","      # axes[i].imshow(np.squeeze((images[i]+1.0)/2.0))\n","      axes[i].imshow(np.squeeze(images[i]))\n","    else:\n","      axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n","    axes[i].set_title(class_dict[label[i]])\n","    plt.setp(axes[i].get_xticklabels(), visible=False)\n","    plt.setp(axes[i].get_yticklabels(), visible=False)\n","  ff.subplots_adjust(wspace=1.)\n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IqRud03ZI5eI","colab_type":"text"},"cell_type":"markdown","source":["## Model configuration\n","- architecture based on VGG16\n","- groups of Conv2D + ReLU, followed by BatchNorm\n","- num_channels, strides, and rates: \n","      * block1 [64, 1, 1] [64, 2, 1] BatchNorm\n","      * block2 [128, 1, 1] [128, 1, 1] BatchNorm\n","      * block3 [256, 1, 1] [256, 1, 1] [256, 2, 1] BatchNorm\n","      * block4 [512, 1, 1] [512, 1, 1] [512, 1, 1] BatchNorm\n","      * block5 [512, 1, 2] [512, 1, 2] [512, 1, 2] BatchNorm\n","      * block6 [512, 1, 2] [512, 1, 2] [512, 1, 2] BatchNorm\n","      * block7 [512, 1, 1] [512, 1, 1] [512, 1, 1] BatchNorm\n","      * block8 deconv, conv, conv\n","      * conv_out\n","      \n","- all kernel shapes: 3x3, except last conv_out (1x1) and deconv (4x4)\n","- padding: SAME\n","- Each convolutional layer is followed by ReLU\n","- The last convolutional layer has num_channels = 313 (number of bins in ab space)"]},{"metadata":{"id":"FDg8cE2B8kHB","colab_type":"code","colab":{}},"cell_type":"code","source":["# define the model\n","class VGG(snt.AbstractModule):\n","  def __init__(self, name=\"vgg\"):\n","    super(VGG, self).__init__(name=name)\n","\n","  def _build(self, inputs, is_training=None, test_local_stats=False):\n","    net = inputs\n","    # block1\n","    conv_num = 1\n","    net = snt.Conv2D(64, 3, stride=1, name='conv' + str(conv_num))(net)\n","    net = tf.nn.relu(net)\n","  \n","    conv_num += 1\n","    net = snt.Conv2D(64, 3, stride=2, name='conv' + str(conv_num))(net)\n","    net = tf.nn.relu(net)\n","    \n","    net = snt.BatchNorm(name='bn1')(net, is_training=is_training, test_local_stats=test_local_stats)\n","\n","    # block2\n","    #### your code here ####\n","\n","    # block3\n","    #### your code here ####\n","    \n","    # block4\n","    #### your code here ####\n","\n","    # block5\n","    #### your code here ####\n","\n","    # block6\n","    #### your code here ####\n","    \n","    # block7\n","    #### your code here ####\n","\n","    # block8\n","    conv_num += 1\n","    net = snt.Conv2DTranspose(256, kernel_shape=4, stride=2, name='conv' + str(conv_num))(net)\n","    net = tf.nn.relu(net)\n","\n","    conv_num += 1    \n","    net = snt.Conv2D(256, 3, stride=1, name='conv' + str(conv_num))(net)\n","    net = tf.nn.relu(net)\n","\n","    conv_num += 1\n","    net = snt.Conv2D(256, 3, stride=1, name='conv' + str(conv_num))(net)\n","    net = tf.nn.relu(net)\n","\n","    # Unary prediction\n","    conv_num += 1\n","    net = snt.Conv2D(313, 1, stride=1, name='conv' + str(conv_num))(net)\n","\n","    return net"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bB9jMO6gHgD_","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_num_params(scope):\n","  total_parameters = 0\n","  for variable in tf.trainable_variables(scope):\n","    # shape is an array of tf.Dimension\n","    shape = variable.get_shape()\n","    variable_parameters = 1\n","    for dim in shape:\n","      variable_parameters *= dim.value\n","    total_parameters += variable_parameters\n","  return total_parameters"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xm-QQKooHu-g","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.variable_scope('model'):\n","  vgg_model = VGG()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Iy-s-S1wID06","colab_type":"code","colab":{}},"cell_type":"code","source":["# use py_func to convert images to Lab space and to embedding space\n","preprocessed_train_batch = tf.py_function(py_preprocess_images_labels, [images_train], [tf.float32, tf.float32, tf.float32])\n","\n","# retrieve lightness L channel\n","train_inputs = preprocessed_train_batch[0]\n","train_inputs.set_shape([batch_size, image_size, image_size, 1])\n","\n","# output will be twice smaller, so downsample lightness to concatenate with predicted ab channels\n","ds_train_inputs = tf.image.resize_nearest_neighbor(train_inputs, [int(image_size/2), int(image_size/2)])\n","\n","# retrieve ground truth (embedded ab channels)\n","train_labels = preprocessed_train_batch[1]\n","train_labels.set_shape([batch_size, image_size, image_size, num_outputs])\n","\n","# retrieve weights to boost pixels depending on colour rarity\n","weights_train = preprocessed_train_batch[2]\n","weights_train.set_shape([batch_size, image_size, image_size])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UelGBU2NfuPP","colab_type":"code","colab":{}},"cell_type":"code","source":["# get predictions from the model during training; is_training=True\n","train_predictions = #### your code here ####\n","print (train_predictions)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vA_XRqdLfp9U","colab_type":"code","colab":{}},"cell_type":"code","source":["# Do the same for validation data\n","preprocessed_valid_batch = tf.py_function(py_preprocess_images_labels, [images_valid], [tf.float32, tf.float32, tf.float32])\n","valid_inputs = preprocessed_valid_batch[0]\n","valid_inputs.set_shape([batch_size, image_size, image_size, 1])\n","\n","ds_valid_inputs = tf.image.resize_nearest_neighbor(valid_inputs, [int(image_size/2), int(image_size/2)])\n","\n","valid_labels = preprocessed_valid_batch[1]\n","valid_labels.set_shape([batch_size, image_size, image_size, num_outputs])\n","\n","weights_valid = preprocessed_train_batch[2]\n","weights_valid.set_shape([batch_size, image_size, image_size])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_eoHzu9tgFBd","colab_type":"code","colab":{}},"cell_type":"code","source":["# get predictions from the model during validation\n","\n","#### your code here ####\n","\n","print (valid_predictions)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BFHi5TghILOg","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get number of parameters in the model. Can you obtain this number by hand?\n","print (\"Total number of parameters of baseline model\")\n","print (get_num_params(\"model\"))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-pcXpRcFIRK0","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_loss(logits=None, labels=None, weights=None):\n","  print (logits)\n","  print (labels)\n","  # labels need to be downsampled\n","  labels = tf.image.resize_nearest_neighbor(labels, [int(image_size/2), int(image_size/2)])\n","  weights = tf.image.resize_nearest_neighbor(tf.expand_dims(weights, axis=-1), [int(image_size/2), int(image_size/2)])\n","  weights = tf.tile(weights, [1, 1, 1, 313])\n","  logits_flatten = tf.reshape(logits, [-1, 313])\n","  labels_flatten = tf.reshape(labels, [-1, 313])\n","  \n","  # get cross entropy loss\n","  g_loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels_flatten, logits_flatten)) / (batch_size)\n","  \n","  # we apply the weights only to gradients, not to output predictions\n","  dl2c = tf.gradients(g_loss, logits)[0]\n","  dl2c = tf.stop_gradient(dl2c)\n","  new_loss = tf.reduce_mean(dl2c * logits * weights)\n","  return new_loss, g_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"a8UBhPdIJYW5","colab_type":"code","colab":{}},"cell_type":"code","source":["# Define train and test loss functions\n","train_loss, train_display_loss = get_loss(labels=train_labels, logits=train_predictions, weights=weights_train)\n","_, valid_loss = get_loss(labels=valid_labels, logits=valid_predictions, weights=weights_valid)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OIcWp8E7Jfyj","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_optimizer(step):\n","  \"\"\"Get the optimizer used for training.\"\"\"\n","  lr_init = 0.00001 # initial value for the learning rate\n","  lr_schedule = (60e3, 100e3, 150e3) # after how many iterations to reduce the learning rate\n","  lr_schedule = tf.cast(lr_schedule, tf.int64)\n","  lr_factor = 0.1 # reduce learning rate by this factor\n","  \n","  \n","  num_epochs = tf.reduce_sum(tf.cast(step >= lr_schedule, tf.float16))\n","  lr = lr_init * lr_factor**num_epochs\n","\n","  # return tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9)\n","  return tf.train.AdamOptimizer(learning_rate=lr, beta2=0.99)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OY8f3MZxJkLv","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create a global step that is incremented during training; useful for e.g. learning rate annealing\n","global_step = tf.train.get_or_create_global_step()\n","\n","# instantiate the optimizer\n","optimizer = get_optimizer(global_step)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Is0yMz4WJoWA","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get training ops\n","training_op = optimizer.minimize(train_loss, global_step)\n","update_ops = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n","training_op = tf.group(training_op, update_ops)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"seLYYZeWJxtb","colab_type":"code","colab":{}},"cell_type":"code","source":["# Function that takes a list of losses and plots them.\n","def plot_losses(loss_list, steps):\n","  display(pl.figure(0))\n","  pl.plot(steps, loss_list, c='b')\n","  time.sleep(1.0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F8eO3H6FJ1OC","colab_type":"code","colab":{}},"cell_type":"code","source":["# Define number of training iterations and reporting intervals\n","TRAIN_ITERS = 200e3 #@param\n","REPORT_TRAIN_EVERY = 10 #@param\n","PLOT_EVERY = 500 #@param\n","REPORT_VALID_EVERY = 1000 #@param\n","VALID_ITERS = 100 #@param"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q3cx8di9-LKL","colab_type":"code","cellView":"form","colab":{}},"cell_type":"code","source":["#@title Plot tensorflow graph\n","def strip_consts(graph_def, max_const_size=32):\n","    \"\"\"Strip large constant values from graph_def.\"\"\"\n","    strip_def = tf.GraphDef()\n","    for n0 in graph_def.node:\n","        n = strip_def.node.add() \n","        n.MergeFrom(n0)\n","        if n.op == 'Const':\n","            tensor = n.attr['value'].tensor\n","            size = len(tensor.tensor_content)\n","            if size > max_const_size:\n","                tensor.tensor_content = \"<stripped %d bytes>\"%size\n","    return strip_def\n","\n","  \n","def show_graph(graph_def=None, max_const_size=32):\n","    \"\"\"Visualize TensorFlow graph. Default to the default graph.\"\"\"\n","    if graph_def is None:\n","      graph_def = tf.get_default_graph()\n","    if hasattr(graph_def, 'as_graph_def'):\n","        graph_def = graph_def.as_graph_def()\n","    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n","    code = \"\"\"\n","        <script src=\"//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js\"></script>\n","        <script>\n","          function load() {{\n","            document.getElementById(\"{id}\").pbtxt = {data};\n","          }}\n","        </script>\n","        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n","        <div style=\"height:600px\">\n","          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n","        </div>\n","    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n","\n","    iframe = \"\"\"\n","        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n","    \"\"\".format(code.replace('\"', '&quot;'))\n","    display(HTML(iframe))\n","\n","# show_graph()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_ZQeAwHXJ4Kh","colab_type":"code","colab":{}},"cell_type":"code","source":["# Train the model within a session\n","train_iter = 0\n","losses = []\n","steps = []\n","flag_plot = True\n","display_out = True\n","with tf.train.MonitoredSession() as sess:\n","  for train_iter in range(int(TRAIN_ITERS)):\n","    _, train_loss_np, inputs, preds, lbls, ims = sess.run([training_op, train_display_loss, train_inputs, train_predictions, labels_train, images_train])\n","    \n","    if (train_iter % REPORT_TRAIN_EVERY) == 0:\n","      losses.append(train_loss_np)\n","      steps.append(train_iter)\n","      # print ('Train loss at iter {0:5d} out of {1:5d} is {2:.2f}'.format(int(train_iter), int(TRAIN_ITERS), train_loss_np))\n","    if ((train_iter+1) % PLOT_EVERY) == 0 and flag_plot:\n","      plot_losses(losses, steps)   \n","      if display_out:\n","          # convert predictions to RGB\n","          out_RGB = embedding2RGB(inputs, preds, rebalance=.38)\n","          # display illuminance inputs\n","          gallery(inputs, lbls)\n","          # display original colour images\n","          gallery(ims, lbls)\n","          # display predictions\n","          gallery(out_RGB, lbls) \n","      \n","    if (train_iter % REPORT_VALID_EVERY) == 0:\n","      avg_loss = 0.0\n","      for test_iter in range(VALID_ITERS):\n","        loss, inputs, preds, lbls, ims = sess.run([valid_loss, valid_inputs, valid_predictions, labels_valid, images_valid])\n","        avg_loss += loss\n","      avg_loss /= (VALID_ITERS)\n","      print ('Test loss at iter {0:5d} out of {1:5d} is {2:.2f}'.format(int(train_iter), int(TRAIN_ITERS), avg_loss))\n","      if display_out:\n","        # convert predictions to RGB\n","        out_RGB = embedding2RGB(inputs, preds, rebalance=.38)\n","        # display illuminance inputs\n","        gallery(inputs, lbls)\n","        # display original colour images\n","        gallery(ims, lbls)\n","        # display predictions\n","        gallery(out_RGB, lbls)        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"cYdZSc3qRhxf","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}